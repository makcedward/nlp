# NLP - Tutorial
Repository to show how NLP can tacke real problem. Including the source code, dataset, state-of-the art in NLP

## Text Processing
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| Tokenization | Word Tokenization |  | [Medium](https://medium.com/@makcedward/nlp-pipeline-word-tokenization-part-1-4b2b547e6a3) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-word_tokenization.ipynb) |
| Tokenization | Sentence Tokenization |  | [Medium](https://medium.com/@makcedward/nlp-pipeline-sentence-tokenization-part-6-86ed55b185e6) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-sentence_tokenization.ipynb) |
| Part of Speech | | | [Medium](https://medium.com/@makcedward/nlp-pipeline-part-of-speech-part-2-b683c90e327d) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-part_of_speech.ipynb) |
| Lemmatization | | | [Medium](https://medium.com/@makcedward/nlp-pipeline-lemmatization-part-3-4bfd7304957) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp_lemmatization.ipynb) |
| Stemming | | | [Medium](https://medium.com/@makcedward/nlp-pipeline-stemming-part-4-b60a319fd52) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-stemming.ipynb) |
| Stop Words | | | [Medium](https://medium.com/@makcedward/nlp-pipeline-stop-words-part-5-d6770df8a936) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-stop_words.ipynb) |
| Phrase Word Recognition | |  |  |
| Spell Checking | Lexicon-based | Peter Norvig algorithm | [Medium](https://towardsdatascience.com/correcting-your-spelling-error-with-4-operations-50bcfd519bb8) [Github](https://github.com/makcedward/nlp/blob/master/sample/util/nlp-util-spell_corrector.ipynb) |
| | Lexicon-based | Symspell | [Medium](https://towardsdatascience.com/essential-text-correction-process-for-nlp-tasks-f731a025fcc3) [Github](https://github.com/makcedward/nlp/blob/master/sample/util/nlp-util-symspell.ipynb) |
| | Machine Translation | Statistical Machine Translation | [Medium](https://towardsdatascience.com/correcting-text-input-by-machine-translation-and-classification-fa9d82087de1) |
| | Machine Translation | Attention | [Medium](https://towardsdatascience.com/fix-your-text-thought-attention-before-nlp-tasks-7dc074b9744f) |
| String Matching | Fuzzywuzzy | | [Medium](https://towardsdatascience.com/how-fuzzy-matching-improve-your-nlp-model-bc617385ad6b) [Github](https://github.com/makcedward/nlp/blob/master/sample/preprocessing/nlp-preprocessing-string_matching-fuzzywuzzy.ipynb) |

## Named Entity Recognition (NER)
| Section | Sub-Section | Research Lab | Story | Paper & Code |
| --- | --- | --- | --- | --- |
| Pattern-based Recognition |  |  | [Medium](https://towardsdatascience.com/pattern-based-recognition-did-help-in-nlp-5c54b4e7a962)  |  |
| Lexicon-based Recognition |  |  | [Medium](https://towardsdatascience.com/step-out-from-regular-expression-for-feature-engineering-134e594f542c) |  |
| Pre-trained NER | Spacy |  | [Medium](https://medium.com/@makcedward/named-entity-recognition-3fad3f53c91e) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-named_entity_recognition.ipynb) |  |
|  | Custom NER |  |  |  |

## Optical Character Recognition (OCR)
| Section | Sub-Section | Research Lab | Story | Paper & Code |
| --- | --- | --- | --- | --- |
| Google Cloud Vision API |  | Google | [Medium](https://towardsdatascience.com/secret-of-google-web-based-ocr-service-fe30eecedd01) | [Paper](https://das2018.cvl.tuwien.ac.at/media/filer_public/85/fd/85fd4698-040f-45f4-8fcc-56d66533b82d/das2018_short_papers.pdf) |


## Text Summarization
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| Extractive Approach |  |  | [Medium](https://medium.com/@makcedward/text-summarization-extractive-approach-567fe4b85c23) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-text_summarization_extractive.ipynb) |
| Abstractive Approach |  |  |  |

## Distance Measurement
| Section | Sub-Section | Description | Link | Paper |
| --- | --- | --- | --- | --- |
| Euclidean Distance, Cosine Similarity and Jaccard Similarity |  |  | [Medium](https://towardsdatascience.com/3-basic-distance-measurement-in-text-mining-5852becff1d7) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-3_basic_distance_measurement_in_text_mining.ipynb) |  |
| Edit Distance | Levenshtein Distance |  | [Medium](https://towardsdatascience.com/measure-distance-between-2-words-by-simple-calculation-a97cf4993305) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-distance-edit_distance.ipynb) |  |
| Word Moving Distance (WMD) |  |  | [Medium](https://towardsdatascience.com/word-distance-between-word-embeddings-cc3e9cf1d632) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-word_mover_distance.ipynb) |
| Manhattan LSTM |  |  | [Medium](https://towardsdatascience.com/text-matching-with-deep-learning-e6aa05333399) | [Paper](http://www.mit.edu/~jonasm/info/MuellerThyagarajan_AAAI16.pdf) |

## Text Representation
| Section | Sub-Section | Research Lab | Story | Paper & Code |
| --- | --- | --- | --- | --- |
| Traditional Method | Bag-of-words (BoW) |  | [Medium](https://towardsdatascience.com/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-bag_of_words.ipynb) |  |
|  | Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) |  | [Medium](https://towardsdatascience.com/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-lsa_lda.ipynb) |  |
| Character Level | Character Embedding | New York University | [Medium](https://medium.com/@makcedward/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-character_embedding.ipynb) | [Paper](https://arxiv.org/pdf/1502.01710v5.pdf) |
| Word Level | Negative Sampling and Hierarchical Softmax |  |  |  |
|  | Word2Vec, GloVe, fastText |  | [Medium](https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-word_embedding.ipynb) |  |
|  | Contextualized Word Vectors (CoVe) | Salesforce | [Medium](https://towardsdatascience.com/replacing-your-word-embeddings-by-contextualized-word-vectors-9508877ad65d) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-embeddings-word-cove.ipynb) | [Paper](http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf) [Code](https://github.com/salesforce/cove) |
|  | Embeddings from Language Models (ELMo) | AI2 | [Medium](https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-embeddings-sentence-elmo.ipynb) | [Paper](https://arxiv.org/pdf/1802.05365.pdf) [Code](https://github.com/allenai/allennlp/) |
|  | Bidirectional Encoder Representations from Transformers (BERT) | Google | [Medium](https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb) | [Paper](https://arxiv.org/pdf/1810.04805) [Code](https://github.com/google-research/bert)| 
|  | Generative Pre-Training (GPT) | Open AI | [Medium](https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b) | [Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) [Code](https://github.com/openai/finetune-transformer-lm)| 
|  | Contextual String Embeddings | Zalando Research | [Medium](https://towardsdatascience.com/contextual-embeddings-for-nlp-sequence-labeling-9a92ba5a6cf0) | [Paper](http://aclweb.org/anthology/C18-1139) [Code](https://github.com/zalandoresearch/flair)| 
|  | Self-Governing Neural Networks (SGNN) | Google | [Medium](https://towardsdatascience.com/embeddings-free-deep-learning-nlp-model-ce067c7a7c93) | [Paper](https://aclweb.org/anthology/D18-1105) | 
| Sentence Level | Skip-thoughts |  | [Medium](https://towardsdatascience.com/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-skip_thoughts.ipynb) | [Paper](https://arxiv.org/abs/1506.06726) [Code](https://github.com/ryankiros/skip-thoughts) |
|  | InferSent |  | [Medium](https://towardsdatascience.com/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-embeddings-sentence-infersent.ipynb) | [Paper](https://arxiv.org/abs/1705.02364) [Code](https://github.com/facebookresearch/InferSent) |
|  | Quick-Thoughts | Google | [Medium](https://towardsdatascience.com/building-sentence-embeddings-via-quick-thoughts-945484cae273) | [Paper](https://arxiv.org/pdf/1803.02893.pdf) [Code](https://github.com/lajanugen/S2V) |
|  | General Purpose Sentence (GenSen) |  | [Medium](https://towardsdatascience.com/learning-generic-sentence-representation-by-various-nlp-tasks-df39ce4e81d7) | [Paper](https://arxiv.org/pdf/1804.00079.pdf) [Code](https://github.com/Maluuba/gensen) |
| Document Level | lda2vec |  | [Medium](https://towardsdatascience.com/combing-lda-and-word-embeddings-for-topic-modeling-fe4a1315a5b4) | [Paper](https://arxiv.org/pdf/1605.02019.pdf) |
|  | doc2vec | Google | [Medium](https://towardsdatascience.com/understand-how-to-transfer-your-paragraph-to-vector-by-doc2vec-1e225ccf102) [Github](https://github.com/makcedward/nlp/blob/master/sample/embeddings/nlp-embeddings-document-doc2vec.ipynb) | [Paper](https://arxiv.org/pdf/1405.4053.pdf) |

## Model Interpretation
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| ELI5, LIME and Skater |  |  | [Medium](https://towardsdatascience.com/3-ways-to-interpretate-your-nlp-model-to-management-and-customer-5428bc07ce15) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-model_interpretation.ipynb) |
| SHapley Additive exPlanations (SHAP) |  |  | [Medium](https://towardsdatascience.com/interpreting-your-deep-learning-model-by-shap-e69be2b47893) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-model_interpretation_shap.ipynb) |
| Anchors |  |  | [Medium](https://towardsdatascience.com/anchor-your-model-interpretation-by-anchors-aa4ed7104032) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-model_interpretation_anchor.ipynb) |

## Myth
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| Using Deep Learning can resolve all problem? |  |  | [Medium](https://medium.com/@makcedward/how-can-use-player-name-to-predict-world-cup-with-80-accuracy-262d076544c4) [Kaggle](https://www.kaggle.com/makcedward/world-cup-prediction-with-80-accuracy-in-dl-model) |

## Source Code 
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| Spellcheck |  |  | [Github](https://github.com/norvig/pytudes) |
| InferSent |  |  | [Github](https://github.com/facebookresearch/InferSent) |