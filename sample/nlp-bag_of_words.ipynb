{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 basic approaches in Bag of Words which are better than Word Embeddings\n",
    "\n",
    "![3 basic approaches in Bag of Word which are better than Word Embedding](https://cdn.pixabay.com/photo/2014/07/04/15/57/hong-kong-383963_960_720.jpg)\n",
    "\n",
    "Photo: https://pixabay.com/en/hong-kong-china-city-chinese-asian-383963/\n",
    "\n",
    "Nowadays, every one is talking about Word (or Character, Sentence, Document) Embeddings. Is Bag of Words still worth using? Should we apply embedding in any scenario?\n",
    "After reading this article, you will know:\n",
    "- Why people say that Word Embedding is the silver bullet?\n",
    "- When does Bag of Words win over Word Embeddings?\n",
    "- 3 basic approaches in Bag of Words\n",
    "- How can we build Bag of Words in a few line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why somebody say that Word Embeddings are the silver bullet?\n",
    "In the-state-of-art of the NLP field, Embedding is the success way to resolve text related problem and outperform Bag of Words (BoW). Indeed, BoW introduced limitations large feature dimension, sparse representation etc. For word embedding, you may check out my previous post. \n",
    "\n",
    "Should we still use BoW? We may better use BoW in some scenarios.\n",
    "\n",
    "# When does Bag of Words win over Word Embeddings?\n",
    "You may still consider to use BoW rather than Word Embedding in the following situations:\n",
    "- Building an baseline model. By using scikit-learn, there is just a few lines of code to build model. Later on, can using Deep Learning to bit it.\n",
    "- If your dataset is small and context is domain specific, BoW may work better than Word Embedding. Context is very domain specific which means that you cannot find corresponding Vector from pre-trained word embedding models (GloVe, fastText etc).\n",
    "\n",
    "# How can we build Bag of Words in a few line?\n",
    "There is 3 simple ways to build BoW model by using traditional powerful ML libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train_raw_df = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_raw_df.data\n",
    "y_train = train_raw_df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Occurrence\n",
    "\n",
    "![](https://cdn.pixabay.com/photo/2014/06/11/18/07/home-366927_960_720.jpg)\n",
    "\n",
    "Photo: https://pixabay.com/en/home-money-euro-calculator-finance-366927/\n",
    "\n",
    "Counting word occurrence. The reason behind of using this approach is that keyword or important signal will occur again and again. So if the number of occurrence represent the importance of word. More frequency means more importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bow</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>way</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word  Count\n",
       "16   of      3\n",
       "26  the      3\n",
       "3   bow      2\n",
       "0   and      1\n",
       "28  way      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"In the-state-of-art of the NLP field, Embedding is the \\\n",
    "success way to resolve text related problem and outperform \\\n",
    "Bag of Words ( BoW ). Indeed, BoW introduced limitations \\\n",
    "large feature dimension, sparse representation etc.\"\n",
    "\n",
    "count_vec = CountVectorizer()\n",
    "count_occurs = count_vec.fit_transform([doc])\n",
    "count_occur_df = pd.DataFrame((count, word) for word, count in zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))\n",
    "count_occur_df.columns = ['Word', 'Count']\n",
    "count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
    "count_occur_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Count Occurrence\n",
    "If you think that high frequency may dominate the result and causing model bias. Normalization can be apply to pipeline easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bow</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>way</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word     Count\n",
       "16   of  0.428571\n",
       "26  the  0.428571\n",
       "3   bow  0.285714\n",
       "0   and  0.142857\n",
       "28  way  0.142857"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"In the-state-of-art of the NLP field, Embedding is the \\\n",
    "success way to resolve text related problem and outperform \\\n",
    "Bag of Words ( BoW ). Indeed, BoW introduced limitations \\\n",
    "large feature dimension, sparse representation etc.\"\n",
    "\n",
    "norm_count_vec = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "norm_count_occurs = norm_count_vec.fit_transform([doc])\n",
    "norm_count_occur_df = pd.DataFrame((count, word) for word, count in zip(\n",
    "    norm_count_occurs.toarray().tolist()[0], norm_count_vec.get_feature_names()))\n",
    "norm_count_occur_df.columns = ['Word', 'Count']\n",
    "norm_count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
    "norm_count_occur_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "TF-IDF take another approach which is believe that high frequency may not able to provide much information gain. In another word, rare words contribute more weights to the model. \n",
    "\n",
    "Word importance will be increased if the number of occurrence within same document (i.e. training record). On the other hand, it will be decreased if it occurs in corpus (i.e. other training records)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bow</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>way</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word     Count\n",
       "16   of  0.428571\n",
       "26  the  0.428571\n",
       "3   bow  0.285714\n",
       "0   and  0.142857\n",
       "28  way  0.142857"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"In the-state-of-art of the NLP field, Embedding is the \\\n",
    "success way to resolve text related problem and outperform \\\n",
    "Bag of Words ( BoW ). Indeed, BoW introduced limitations \\\n",
    "large feature dimension, sparse representation etc.\"\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_count_occurs = tfidf_vec.fit_transform([doc])\n",
    "tfidf_count_occur_df = pd.DataFrame((count, word) for word, count in zip(\n",
    "    tfidf_count_occurs.toarray().tolist()[0], tfidf_vec.get_feature_names()))\n",
    "tfidf_count_occur_df.columns = ['Word', 'Count']\n",
    "tfidf_count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
    "tfidf_count_occur_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['a', 'an', 'the']\n",
    "\n",
    "# Basic cleansing\n",
    "def cleansing(text):\n",
    "    # Tokenize\n",
    "    tokens = text.split(' ')\n",
    "    # Lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # Remove stop words\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# All-in-one preproce\n",
    "def preprocess_x(x):\n",
    "    processed_x = [cleansing(text) for text in x]\n",
    "    \n",
    "    return processed_x\n",
    "\n",
    "def build_model(mode):\n",
    "    # Intent to use default paramaters for show case\n",
    "    vect = None\n",
    "    if mode == 'count':\n",
    "        vect = CountVectorizer()\n",
    "    elif mode == 'tf':\n",
    "        vect = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "    elif mode == 'tfidf':\n",
    "        vect = TfidfVectorizer()\n",
    "    else:\n",
    "        raise ValueError('Mode should be either count or tfidf')\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('vect', vect),\n",
    "        ('clf' , LogisticRegression(solver='newton-cg',n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "def pipeline(x, y, mode):\n",
    "    processed_x = preprocess_x(x)\n",
    "    \n",
    "    model_pipeline = build_model(mode)\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    scores = cross_val_score(model_pipeline, processed_x, y, cv=cv, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let check number of vocabulary we need to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Vocabulary: 130107\n"
     ]
    }
   ],
   "source": [
    "x = preprocess_x(x_train)\n",
    "y = y_train\n",
    "    \n",
    "model_pipeline = build_model(mode='count')\n",
    "model_pipeline.fit(x, y)\n",
    "\n",
    "print('Number of Vocabulary: %d'% (len(model_pipeline.named_steps['vect'].get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Count Vectorizer------\n",
      "Accuracy: 0.8892 (+/- 0.0198)\n",
      "Using TF Vectorizer------\n",
      "Accuracy: 0.8071 (+/- 0.0110)\n",
      "Using TF-IDF Vectorizer------\n",
      "Accuracy: 0.8917 (+/- 0.0072)\n"
     ]
    }
   ],
   "source": [
    "print('Using Count Vectorizer------')\n",
    "model_pipeline = pipeline(x_train, y_train, mode='count')\n",
    "\n",
    "print('Using TF Vectorizer------')\n",
    "model_pipeline = pipeline(x_train, y_train, mode='tf')\n",
    "\n",
    "print('Using TF-IDF Vectorizer------')\n",
    "model_pipeline = pipeline(x_train, y_train, mode='tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "From previous experience, I tried to tackle the problem of classifying product category by giving a short description. For example, given \"Fresh Apple\" and the expected category is \"Fruit\". Already able to have 80+ accuracy by using count occurrence approach only. \n",
    "\n",
    "In this case, since the number of word per training record is just a few words (from 2 words to 10 words). It may not be a good idea to use Word Embedding as there is no much neighbor (words) for training the vectors. \n",
    "On the other hand, scikit-learn provides other parameter to further tune the model input. You may need to take a look on the following features\n",
    "- ngram_range: Rather than using single word, ngram can be defined as well\n",
    "- binary: Besides counting occurrence, binary representation can be chosen.\n",
    "- max_features: Instead of using all words, max number of word can be chosen to reduce the model complexity and size.\n",
    "\n",
    "Also, some preprocessing steps can be executed within above library rather than handle it by yourself. For example, stop word removal, lower case etc. To have a better flexibility, I will use my own code to finish the preprocessing steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
